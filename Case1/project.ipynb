{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_cleaning import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_creator(methods, models_names: list, columns_names: list, k1: int):\n",
    "    header = pd.MultiIndex.from_product([methods, models_names, columns_names])\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    df['KFold'] = np.arange(1, k1+1)\n",
    "    df.set_index('KFold', inplace=True)\n",
    "    return df\n",
    "\n",
    "def twolevelcv(df, k1: int, k2: int, models: list, params: dict, rs: int, fill_methods: list, std_method: list):\n",
    "    \"\"\"Allows to compute two level crossvalidation.\n",
    "\n",
    "    Args:\n",
    "        X (np.array): Features (numeric)\n",
    "        y (np.array): Class (objective variable)\n",
    "        k1 (int): Nº of outer folds\n",
    "        k2 (int): Nº of inner folds\n",
    "        models (list): List of models for comparison\n",
    "        params (dict): Dictionary including the set of parameters. In this case we only tune 1 parameter per model.\n",
    "        rs (int): Random state\n",
    "    Returns:\n",
    "        df: Dataframe\n",
    "    \"\"\"\n",
    "    k = 0\n",
    "    min_error = np.inf\n",
    "    min_param = None\n",
    "    names = [type(m).__name__ for m in models]\n",
    "    col_names = ['Param. Value', 'Error']\n",
    "    results_df = dataset_creator(fill_methods, names, col_names, k1)\n",
    "    kf1 = KFold(k1, shuffle = True, random_state=rs)\n",
    "    X_raw = df.drop('y', axis=1)\n",
    "    y = df['y']\n",
    "    combs = list(itertools.product(fill_methods,std_method,zip(names,models)))\n",
    "    it_combs = len(combs)\n",
    "    \n",
    "    best_fill = [0]*k1\n",
    "    best_std = [0]*k1\n",
    "    best_name_model = [0]*k1\n",
    "    errors_out = [0]*k1\n",
    "    \n",
    "    # first level split\n",
    "    for z,(train_idx1, test_idx1) in enumerate(kf1.split(X_raw, y)):\n",
    "        error_test = {}\n",
    "        k += 1\n",
    "        kf2 = KFold(k2, shuffle = True, random_state=rs)\n",
    "        print(f'Computing KFold {k}/{k1}...')\n",
    "        err = [[] for pp in range(it_combs)]\n",
    "        # second level split\n",
    "        for train_idx2, test_idx2 in tqdm(kf2.split(X_raw.iloc[train_idx1, :], y[train_idx1]), total = k2):\n",
    "            for i in range(it_combs):\n",
    "                method = combs[i][0]\n",
    "                std_method = combs[i][1]\n",
    "                name,model = combs[i][2]\n",
    "                X_train,X_test = transform_data(X_raw.iloc[train_idx2, :],X_raw.iloc[test_idx2, :] \\\n",
    "                                                , fill_method=method, std_method=std_method)\n",
    "                y_train = y[train_idx2]\n",
    "                y_test = y[test_idx2]\n",
    "                grid = list(ParameterGrid(params[name]))\n",
    "                n_p = len(grid)\n",
    "                if err[i] == []:\n",
    "                    err[i] = [0]*n_p\n",
    "                for j in range(n_p):\n",
    "                    p_ = grid[j]\n",
    "                    model = model.set_params(**p_)\n",
    "                    # train the model\n",
    "                    model.fit(X_train, y_train)\n",
    "                    # evaluate performance\n",
    "                    pred2_test = model.predict(X_test)\n",
    "                    error = mse(pred2_test, y_test,squared = False)\n",
    "                    err[i][j] += error*len(test_idx2)/len(train_idx1)\n",
    "        # inner cv has finished, choose model and param\n",
    "        best_err = np.inf\n",
    "        i_best = None\n",
    "        j_best = None\n",
    "        for i in range(it_combs):\n",
    "            for j in range(len(err[i])):\n",
    "                if err[i][j] < best_err:\n",
    "                    i_best = i\n",
    "                    j_best = j\n",
    "                    best_err = err[i][j]\n",
    "        method = combs[i_best][0]\n",
    "        std_method = combs[i_best][1]\n",
    "        name,model = combs[i_best][2]\n",
    "        grid = list(ParameterGrid(params[name]))\n",
    "        p_ = grid[j_best]\n",
    "        model = model.set_params(**p_)\n",
    "\n",
    "        X_tr,X_te = transform_data(X_raw.iloc[train_idx1, :], X_raw.iloc[test_idx1, :],  \\\n",
    "                                   fill_method=method, std_method=std_method)\n",
    "        y_te = y[test_idx1]\n",
    "        y_tr = y[train_idx1]\n",
    "        model.fit(X_tr,y_tr)\n",
    "        pred = model.predict(X_te)\n",
    "        error = mse(pred,y_te,squared = False)\n",
    "        \n",
    "        best_fill[z] = method\n",
    "        best_std[z] = std_method\n",
    "        best_name_model[z] = (name,model)\n",
    "        print(f\"fill method: {method}, std_method: {std_method}, model: {name} with parameter: {p_}\")\n",
    "        print(f\"error: {error}\")\n",
    "        errors_out[z] = error\n",
    "    # results_df.loc(axis = 1)[method, name, 'Error'][k] = error_test[idx]\n",
    "    # results_df.loc(axis = 1)[method, name, 'Param. Value'][k] = min_param\n",
    "    return best_fill,best_std,best_name_model,errors_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data flow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing KFold 1/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [05:11<00:00, 31.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill method: mean, std_method: 30, model: Ridge with parameter: {'alpha': 18.71855294965579, 'fit_intercept': False}\n",
      "error: 23.30902464121164\n",
      "Computing KFold 2/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 10/10 [05:12<00:00, 31.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fill method: mean, std_method: 30, model: Ridge with parameter: {'alpha': 18.71855294965579, 'fit_intercept': False}\n",
      "error: 25.109436645611428\n",
      "Computing KFold 3/10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████▍                 | 6/10 [03:23<02:15, 33.94s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m k1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m     39\u001b[0m k2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 40\u001b[0m best_fill,best_std,best_name_model,errors_out \u001b[38;5;241m=\u001b[39m \u001b[43mtwolevelcv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_raw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_methods\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_methods\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstd_methods\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 64\u001b[0m, in \u001b[0;36mtwolevelcv\u001b[0;34m(df, k1, k2, models, params, rs, fill_methods, std_method)\u001b[0m\n\u001b[1;32m     62\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mp_)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# train the model\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# evaluate performance\u001b[39;00m\n\u001b[1;32m     66\u001b[0m pred2_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[0;32m~/anaconda3/envs/CDA/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:1134\u001b[0m, in \u001b[0;36mRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1125\u001b[0m _accept_sparse \u001b[38;5;241m=\u001b[39m _get_valid_accept_sparse(sparse\u001b[38;5;241m.\u001b[39missparse(X), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolver)\n\u001b[1;32m   1126\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[1;32m   1127\u001b[0m     X,\n\u001b[1;32m   1128\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1133\u001b[0m )\n\u001b[0;32m-> 1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/CDA/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:900\u001b[0m, in \u001b[0;36m_BaseRidge.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    897\u001b[0m         \u001b[38;5;66;03m# for dense matrices or when intercept is set to 0\u001b[39;00m\n\u001b[1;32m    898\u001b[0m         params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 900\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_ridge_regression\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    916\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_intercept(X_offset, y_offset, X_scale)\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/CDA/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:708\u001b[0m, in \u001b[0;36m_ridge_regression\u001b[0;34m(X, y, alpha, sample_weight, solver, max_iter, tol, verbose, positive, random_state, return_n_iter, return_intercept, X_scale, X_offset, check_input, fit_intercept)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m         coef \u001b[38;5;241m=\u001b[39m \u001b[43m_solve_cholesky\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m linalg\u001b[38;5;241m.\u001b[39mLinAlgError:\n\u001b[1;32m    710\u001b[0m         \u001b[38;5;66;03m# use SVD solver if matrix is singular\u001b[39;00m\n\u001b[1;32m    711\u001b[0m         solver \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/CDA/lib/python3.10/site-packages/sklearn/linear_model/_ridge.py:216\u001b[0m, in \u001b[0;36m_solve_cholesky\u001b[0;34m(X, y, alpha)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m one_alpha:\n\u001b[1;32m    215\u001b[0m     A\u001b[38;5;241m.\u001b[39mflat[:: n_features \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m alpha[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpos\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverwrite_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     coefs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty([n_targets, n_features], dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/CDA/lib/python3.10/site-packages/scipy/linalg/_basic.py:40\u001b[0m, in \u001b[0;36msolve\u001b[0;34m(a, b, sym_pos, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rcond \u001b[38;5;241m<\u001b[39m E:\n\u001b[1;32m     35\u001b[0m         warn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIll-conditioned matrix (rcond=\u001b[39m\u001b[38;5;132;01m{:.6g}\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     36\u001b[0m              \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult may not be accurate.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(rcond),\n\u001b[1;32m     37\u001b[0m              LinAlgWarning, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msolve\u001b[39m(a, b, sym_pos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lower\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, overwrite_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     41\u001b[0m           overwrite_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, check_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, assume_a\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     42\u001b[0m           transposed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m    Solves the linear equation set ``a @ x == b`` for the unknown ``x``\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    for square `a` matrix.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m \n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Flags for 1-D or N-D right-hand side\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "df_raw, df_Xn_raw = load_data()\n",
    "#std_methods = ['standard','minmax','maxabs','robust'] + list(range(1,81))\n",
    "#fill_methods = ['mean', 'median']\n",
    "\n",
    "fill_methods = ['mean']\n",
    "std_methods = list(range(20,81,5))\n",
    "\n",
    "random_state = 3\n",
    "params = {}\n",
    "lam = np.logspace(-2, 5, 1000)\n",
    "lam_1 = np.logspace(-2, 5, 100)\n",
    "# C = [200000000, 10000000, 0.1519911082952933, 0.2848035868435805 ]\n",
    "params['DummyRegressor'] = {'strategy': ['mean', 'median']}\n",
    "params['LinearRegression'] = {'fit_intercept': [True, False]}\n",
    "params['Ridge'] = {'alpha': lam, 'fit_intercept': [True, False]}\n",
    "params['Lasso'] = {'alpha': lam_1, 'fit_intercept': [True, False], 'max_iter': [1000]}\n",
    "params['ElasticNet'] = {'alpha': lam_1, 'l1_ratio': [0.1, 0.5], 'fit_intercept': [True,False], 'max_iter':[1000]}\n",
    "params['RandomForestRegressor'] = {'n_estimators':[10, 50], 'max_depth':[None ,5], 'min_samples_split':[2 ,10], 'random_state':[random_state]}\n",
    "params['GradientBoostingRegressor'] = {'n_estimators':[10 ,50], 'learning_rate':[0.01 ,0.1], 'max_depth':[3 ,5], 'min_samples_split':[2 ,10], 'random_state':[random_state]}\n",
    "params['KNeighborsRegressor'] = {'n_neighbors':list(range(1,80)),'n_jobs': [-1],'weights':['uniform', 'distance'],'p':[1,2]}\n",
    "\n",
    "# Create a list of models\n",
    "models = [Ridge(random_state=random_state),\n",
    "          DummyRegressor()]\n",
    "\"\"\",\n",
    "          LinearRegression(),\n",
    "          Lasso(random_state=random_state),\n",
    "          ElasticNet(random_state=random_state),\n",
    "          RandomForestRegressor(random_state=random_state),\n",
    "          GradientBoostingRegressor(random_state=random_state),\n",
    "          KNeighborsRegressor()\n",
    "         ]\"\"\"\n",
    "k1 = 10\n",
    "k2 = 10\n",
    "best_fill,best_std,best_name_model,errors_out = twolevelcv(df_raw, k1=k1, k2=k2, models=models,params=params, rs=random_state, fill_methods=fill_methods, std_method = std_methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29.408603294608803, 4.069558369072548)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(errors_out),np.std(errors_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only take the first two features.\n",
    "y = iris.target\n",
    "X_reduced = PCA(n_components=3).fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.74743191e-15, -8.70414851e-16, -3.28626015e-16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(X_reduced,axis = 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
